{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da14338f",
   "metadata": {},
   "source": [
    "# Adversarial Learning - Notes&Code - Jiacheng Zhang\n",
    "\n",
    "#### Reference: \n",
    "#### https://arxiv.org/abs/2202.10377\n",
    "#### https://arxiv.org/abs/1706.06083\n",
    "#### https://github.com/yogeshbalaji/Adversarial-training\n",
    "#### https://github.com/Harry24k/PGD-pytorch\n",
    "#### https://github.com/DengpanFu/RobustAdversarialNetwork\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c787382",
   "metadata": {},
   "source": [
    "# 1.Types of adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03e7f2",
   "metadata": {},
   "source": [
    "#### Evasion: manipulating data  to avoid detection (e.g.  image-based spam in which the spam content is embedded within an attached image to evade textual analysis by anti-spam filters)\n",
    "Evasion attacks can be generally split into two different categories: black box attacks and white box attacks.\n",
    "#### Poisoning: injection malicious training samples to disrupt retraining (e.g. for content recommendation or natural language models, especially given the ubiquity of fake accounts.)\n",
    "#### Model stealing: extraction (e.g. extracting a sufficient amount of data from the model to enable the complete reconstruction of the model)\n",
    "#### Inference: leveraging over-generalization on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013de41",
   "metadata": {},
   "source": [
    "# 2 Threat Modelling for Adversarial Learning Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783f302",
   "metadata": {},
   "source": [
    "### 2.1 Adversarial Threat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23b086",
   "metadata": {},
   "source": [
    "#### 2.1.1 The attack surface: the point in which the attack takes place. (mostly occur during the input data collection and processing in order to exploit the vulnerabilities in the model without corrupting it)\n",
    "#### 2.1.2 Adversarial capability: the capacities of an adversary outline the sort of attacks available to them. These relate to what part of the process they have access to, be it the training phase or the inference phase.\n",
    "- if the adversary has the ability to insert themselves into the training phase, they have the capability to learn to influence the training of, and therefore corrupt, the model iteself. (e.g. injection/modification on training set, logic corruption - tamper with the actuall learning logic of the model)\n",
    "- In the inference phase, attacks do not attempt to corrupt the model itself, but rather to fool it into producing incorrect outputs, by exploiting inherent vulnerabilities. (e.g. white-box/black-box attacks depending on the adversary's access to information)\n",
    "\n",
    "#### 2.1.3 Adversarial goals: they model the goals of potential adversaries using the classic cyber security mode, CIA (confidentiality, integrity and availability)\n",
    "- confidentiality: e.g., in the context of a financial system, the model itself may be regarded as confidential intellectual property, or likewise in the context of a medical system, the training set may contain confidential data used to train it.\n",
    "- integrity: attacks on integrity aim to cause the model to behave in an unintelligent and incorrect manner thereby undermining the integrity of the model. E.g., causing the misclassification of certain inputs to the model or reducing the confidence the model has in its output.\n",
    "- availability: attacks on availability look to threat access to output of models. E.g., Attempts to reduce the access to the model, via denial-of-service attacks, or model's performance metrics such as speed so it becomes inconvenient to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855b378",
   "metadata": {},
   "source": [
    "### 2.2 Intelligent Security Detection System Threat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a075c9",
   "metadata": {},
   "source": [
    "- these systems can generally be attacked in the training stage by poisoning the training set or by logic corruption. Similarly, attacks in the inference stage would aim to evade detection ot potentially trigger false positives. These false positives may trigger unnecessary reactions by the system, potentially causing unwanted responses such as the wiping of important data. A stream of false positives may also strain the system, reducing its performance, and as such, its availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843f8cc",
   "metadata": {},
   "source": [
    "### 2.3 Attack Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16382717",
   "metadata": {},
   "source": [
    "#### 2.3.1 Adversarial falsification: distinguishes between whether the adversary aims to produce a false positive attack or false negative and what this means for the target system.\n",
    "- false positive attack: results in negative samples being incorrectly classified as positive. E.g., In the case of an intrusion detection system this may result in the inappropriate triggering of a response to the perceived attack such as deletion of non-threatening or important information.\n",
    "- false negative attack: also known as evasion attacks, using the same example of an intrusion detection system, the true intruder or malware is classified as benign and left completely undetected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91e667",
   "metadata": {},
   "source": [
    "#### 2.3.2 Adversarial specificity: differentiates between targeted and non-targeted attacks and usually relates to the case of a multiclass classification.\n",
    "- targeted attacks: look to guide the output of the model in a certain direction to a specific class.\n",
    "- non-targeted attacks: aim to classify an adversarial example as any class other than the original.(easier to implement and are either realised by reducing the probability the model classifies correctly or by selecting one from numerous targeted approaches, the one with the slightest perturbation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dff06e",
   "metadata": {},
   "source": [
    "#### 2.3.3 Adversarial frequency: concerns whether the attack is a one-time approached or an iterative one, which updates a number of times to better optimize the attack.\n",
    "- an iterative one tend to perform better, however come with extra cost, requiring greater computational time.\n",
    "- in some cases, the one-time attacks are sufficient or even the only feasible option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82513d60",
   "metadata": {},
   "source": [
    "#### 2.3.4 White-box  vs Black-box: attacks in this case can be split into two distinct categories based on the level of detail they require of the target model\n",
    "- white-box attack: one which requires an in-depth understanding of the target model, the mechanics of the model are transparent. The attacker has full knowledge of the system and has access to vital information including details such as the network architecture, parameters, hyper parameters, training data as well as the ability to gather gradients, and prediction results.\n",
    "- black-box attack: attacks the model with no knowledge of the inner workings of the model.A black box attacker simply requires access to the model in order to query it to produce the prediction result.(e.g., an attacker may be able to produce a surrogate model which acts in accordance with the inputs and outputs of the target model. In this way, the attacker has full access to the inner workings of their surrogate model and can therefore successfullyu conduct white box attacks on it by creating adversarial samples, then these adversarial samples can be used to conduct a black-box attack against the target model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3668d5",
   "metadata": {},
   "source": [
    "# 3. Code Re-implementation of https://arxiv.org/abs/1706.06083\n",
    "- Based on MNIST Data only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc883bf5",
   "metadata": {},
   "source": [
    "### 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b90ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd688b",
   "metadata": {},
   "source": [
    "### 2. Download MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedd7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor() # ToTensor : [0, 255] -> [0, 1]\n",
    "])\n",
    "\n",
    "trainingData = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testingData = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batchSize = 200\n",
    "epoch = 50\n",
    "\n",
    "train_set = DataLoader(dataset=trainingData,batch_size=batchSize, shuffle=True)\n",
    "test_set = DataLoader(dataset=testingData,batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5688e33",
   "metadata": {},
   "source": [
    "### 3. Show Part of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd012f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.figure(figsize = (5, 15))\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment below to see the images\n",
    "\n",
    "# for images, labels in test_set:\n",
    "#     images = images.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     imshow(make_grid(images.cpu().data, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a51ba",
   "metadata": {},
   "source": [
    "### 4. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6d7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    \"\"\" Construct basic MnistModel for mnist adversal attack \"\"\"\n",
    "    def __init__(self, re_init=False, has_dropout=False):\n",
    "        super(MnistModel, self).__init__()\n",
    "        self.re_init = re_init\n",
    "        self.has_dropout = has_dropout\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.fc1 = nn.Linear(7*7*64, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        if self.has_dropout:\n",
    "            self.dropout = nn.Dropout()\n",
    "\n",
    "        if self.re_init:\n",
    "            self._init_params(self.conv1)\n",
    "            self._init_params(self.conv2)\n",
    "            self._init_params(self.fc1)\n",
    "            self._init_params(self.fc2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.has_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eea015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Models for Different Attacks\n",
    "ori_model = MnistModel().cuda()\n",
    "fgsm_model = MnistModel().cuda()\n",
    "pgd_model = MnistModel().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b827f3",
   "metadata": {},
   "source": [
    "### 5. Define a Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aada5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "ori_optimizer = optim.SGD(ori_model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "fgsm_optimizer = optim.SGD(fgsm_model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "pgd_optimizer = optim.SGD(pgd_model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778350ee",
   "metadata": {},
   "source": [
    "### 6. Define the training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f597723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e7c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer, adversarial):\n",
    "    losses = AverageMeter()\n",
    "    start_time = time.time()\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_set:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            if adversarial=='FGSM':\n",
    "                images = FGSM(model, images, labels, 0.1)\n",
    "            elif adversarial=='PGD':\n",
    "                images = PGD(model, images, labels)\n",
    "            \n",
    "            output = model(images).cuda()\n",
    "            loss = F.cross_entropy(output, labels).cuda()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, prediction = torch.max(output, dim=1)\n",
    "            correct += (prediction == labels).sum()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            losses.update(loss.data.item(), images.size(0))\n",
    "\n",
    "        acc = (float(correct) / total) * 100\n",
    "        if i % 10 == 0:\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            message = 'Epoch {}, Time: {}s, Loss: {}, Train Accuracy: {}%'.format(i+1, batch_time, loss.item(), acc)\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274e76c",
   "metadata": {},
   "source": [
    "### 7. Define the testing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e66307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, adversarial):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_set:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        if adversarial=='FGSM':\n",
    "            images = FGSM(model,images,labels,0.1)\n",
    "        elif adversarial=='PGD':\n",
    "            images = PGD(model, images, labels)\n",
    "            \n",
    "        # compute output\n",
    "        output = model(images).cuda()\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        correct += (pred == labels).sum()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = (float(correct) / total) * 100\n",
    "    message = 'Test Accuracy: {}%'.format(accuracy)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd34eb8",
   "metadata": {},
   "source": [
    "### 8. Natural Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0ec4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Time: 5.669363021850586s, Loss: 0.1208600103855133, Train Accuracy: 82.13666666666667%\n",
      "Epoch 11, Time: 48.821776390075684s, Loss: 0.026549018919467926, Train Accuracy: 99.31333333333333%\n",
      "Epoch 21, Time: 92.3618688583374s, Loss: 0.005918064620345831, Train Accuracy: 99.76333333333334%\n",
      "Epoch 31, Time: 135.0185136795044s, Loss: 0.00995405949652195, Train Accuracy: 99.855%\n",
      "Epoch 41, Time: 178.35860657691956s, Loss: 0.009199175052344799, Train Accuracy: 99.93833333333333%\n"
     ]
    }
   ],
   "source": [
    "train(ori_model,epoch,ori_optimizer, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f672355",
   "metadata": {},
   "source": [
    "### 9. Apply adversarial training with FGSM Attack\n",
    "\n",
    "$$ x^* = x + \\epsilon \\cdot sgn(\\triangledown_x L(\\theta, x, y)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95624c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def FGSM(model, images, labels, epsilon):\n",
    "    \n",
    "    images.requires_grad = True\n",
    "    \n",
    "    output = model(images).cuda()\n",
    "            \n",
    "    loss = F.cross_entropy(output, labels).cuda()\n",
    "            \n",
    "    loss.backward()\n",
    "            \n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = images.grad.sign()\n",
    "    \n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = images + epsilon*sign_data_grad\n",
    "    \n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    \n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "844c3f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Time: 6.0473597049713135s, Loss: 0.4133017659187317, Train Accuracy: 75.07000000000001%\n",
      "Epoch 11, Time: 66.52704954147339s, Loss: 0.0509895384311676, Train Accuracy: 97.485%\n",
      "Epoch 21, Time: 125.90949821472168s, Loss: 0.03253510594367981, Train Accuracy: 98.49333333333334%\n",
      "Epoch 31, Time: 184.2856090068817s, Loss: 0.03364289551973343, Train Accuracy: 99.14%\n",
      "Epoch 41, Time: 245.06063151359558s, Loss: 0.018948663026094437, Train Accuracy: 99.52499999999999%\n"
     ]
    }
   ],
   "source": [
    "train(fgsm_model,epoch,fgsm_optimizer,'FGSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2fa3c",
   "metadata": {},
   "source": [
    "### 10.  Apply adversarial training with PGD Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb4ac6",
   "metadata": {},
   "source": [
    "$$x^{t+1} = \\Pi_{x+S}(x^t+\\alpha\\cdot sgn(\\triangledown_x L(\\theta, x, y)))$$\n",
    "* $S$ : a set of allowed perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aea2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD(model, images, labels, eps=0.3, alpha=8/255, iters=10) :\n",
    "    \n",
    "    # random initialization\n",
    "    ori_images = images.data\n",
    "    x_adv = images +  ori_images.new(ori_images.size()).uniform_(-eps, eps)\n",
    "        \n",
    "    for i in range(iters) :    \n",
    "        x_adv.requires_grad = True\n",
    "        \n",
    "        output = model(x_adv)\n",
    "        \n",
    "        loss = F.cross_entropy(output, labels).cuda()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        x_adv = x_adv.detach() + alpha * x_adv.grad.sign()\n",
    "        \n",
    "        # projection\n",
    "        x_adv = torch.min(torch.max(x_adv, ori_images - eps), ori_images + eps)\n",
    "            \n",
    "        x_adv.clamp_(0, 1)\n",
    "        \n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87b1c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Time: 19.581053495407104s, Loss: 0.6096667647361755, Train Accuracy: 58.36666666666667%\n",
      "Epoch 11, Time: 216.9792833328247s, Loss: 0.11895622313022614, Train Accuracy: 94.91833333333334%\n",
      "Epoch 21, Time: 414.06174325942993s, Loss: 0.03309975564479828, Train Accuracy: 96.78999999999999%\n",
      "Epoch 31, Time: 611.2263371944427s, Loss: 0.07283598184585571, Train Accuracy: 97.80499999999999%\n",
      "Epoch 41, Time: 807.3649859428406s, Loss: 0.05246616527438164, Train Accuracy: 98.53333333333333%\n"
     ]
    }
   ],
   "source": [
    "train(pgd_model,epoch, pgd_optimizer, 'PGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e3f30f",
   "metadata": {},
   "source": [
    "### 11. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18acbc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Training: \n",
      "\n",
      "on normal test samples: \n",
      "Test Accuracy: 99.15%\n",
      "\n",
      "on FGSM samples: \n",
      "Test Accuracy: 80.39%\n",
      "\n",
      "on PGD samples: \n",
      "Test Accuracy: 0.06%\n",
      "--------------------\n",
      "FGSM Adversarial Training: \n",
      "\n",
      "on normal test samples: \n",
      "Test Accuracy: 99.39%\n",
      "\n",
      "on FGSM samples: \n",
      "Test Accuracy: 96.87%\n",
      "\n",
      "on PGD samples: \n",
      "Test Accuracy: 4.47%\n",
      "--------------------\n",
      "PGD Adversarial Training: \n",
      "\n",
      "on normal test samples: \n",
      "Test Accuracy: 99.11999999999999%\n",
      "\n",
      "on FGSM samples: \n",
      "Test Accuracy: 98.04%\n",
      "\n",
      "on PGD samples: \n",
      "Test Accuracy: 94.38%\n"
     ]
    }
   ],
   "source": [
    "# Testing natural model on different samples\n",
    "print('Natural Training: \\n')\n",
    "print('on normal test samples: ')\n",
    "evaluate(ori_model, None)\n",
    "print('\\non FGSM samples: ')\n",
    "evaluate(ori_model, 'FGSM')\n",
    "print('\\non PGD samples: ')\n",
    "evaluate(ori_model, 'PGD')\n",
    "print('--------------------')\n",
    "\n",
    "\n",
    "# Testing FGSM model on different samples\n",
    "print('FGSM Adversarial Training: \\n')\n",
    "print('on normal test samples: ')\n",
    "evaluate(fgsm_model, None)\n",
    "print('\\non FGSM samples: ')\n",
    "evaluate(fgsm_model, 'FGSM')\n",
    "print('\\non PGD samples: ')\n",
    "evaluate(fgsm_model, 'PGD')\n",
    "print('--------------------')\n",
    "\n",
    "\n",
    "# Testing PGD model on different samples\n",
    "print('PGD Adversarial Training: \\n')\n",
    "print('on normal test samples: ')\n",
    "evaluate(pgd_model, None)\n",
    "print('\\non FGSM samples: ')\n",
    "evaluate(pgd_model, 'FGSM')\n",
    "print('\\non PGD samples: ')\n",
    "evaluate(pgd_model, 'PGD')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
